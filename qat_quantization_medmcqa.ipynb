{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e609c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "from torch.quantization import get_default_qat_qconfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8801bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0c548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\jarvis\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Паша\\.cache\\huggingface\\hub\\models--facebook--opt-125m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/opt-125m\"\n",
    "DATASET_NAME = \"medmcqa\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d65b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.qconfig = get_default_qat_qconfig('fbgemm')  # Use fbgemm for x86 CPUs or 'qnnpack' for mobile\n",
    "model = torch.quantization.prepare_qat(model, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6aed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\jarvis\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Паша\\.cache\\huggingface\\hub\\datasets--medmcqa. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 182822/182822 [00:00<00:00, 243672.66 examples/s]\n",
      "Generating test split: 100%|██████████| 6150/6150 [00:00<00:00, 392187.70 examples/s]\n",
      "Generating validation split: 100%|██████████| 4183/4183 [00:00<00:00, 267834.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b08085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qat(model, tokenizer, dataset, num_epochs=1, max_steps=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(dataset[\"train\"]):\n",
    "            if i >= max_steps:\n",
    "                break\n",
    "\n",
    "            question = batch[\"question\"]\n",
    "            options = [batch[\"opa\"], batch[\"opb\"], batch[\"opc\"], batch[\"opd\"]]\n",
    "            answer_idx = batch[\"cop\"]\n",
    "\n",
    "            prompt = f\"Question: {question}\\nOptions:\\nA. {options[0]}\\nB. {options[1]}\\nC. {options[2]}\\nD. {options[3]}\\nAnswer: {['A','B','C','D'][answer_idx]}\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                logging.info(f\"Step {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Convert to quantized model after training\n",
    "    model = torch.quantization.convert(model, inplace=False)\n",
    "    return model\n",
    "\n",
    "# Evaluate function\n",
    "def evaluate_qat_model(model, tokenizer, dataset, limit=200):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataset[\"validation\"]):\n",
    "            if i >= limit:\n",
    "                break\n",
    "\n",
    "            question = batch[\"question\"]\n",
    "            options = [batch[\"opa\"], batch[\"opb\"], batch[\"opc\"], batch[\"opd\"]]\n",
    "            answer_idx = batch[\"cop\"]\n",
    "\n",
    "            prompt = f\"Question: {question}\\nOptions:\\nA. {options[0]}\\nB. {options[1]}\\nC. {options[2]}\\nD. {options[3]}\\nAnswer:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "            \n",
    "            outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "            pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answer_letter = pred_text.strip().upper().split(\"Answer:\")[-1].strip()[0]\n",
    "\n",
    "            label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "            if answer_letter in label_map:\n",
    "                predictions.append(label_map[answer_letter])\n",
    "            else:\n",
    "                predictions.append(None)\n",
    "\n",
    "            references.append(answer_idx)\n",
    "\n",
    "    # Filter out None predictions\n",
    "    filtered = [(p, r) for p, r in zip(predictions, references) if p is not None]\n",
    "    predictions, references = zip(*filtered) if filtered else ([], [])\n",
    "\n",
    "    accuracy = accuracy_score(references, predictions) if predictions else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb0b86d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1/1\n",
      "INFO:root:Step 0, Loss: 12.0137\n",
      "INFO:root:Step 10, Loss: 5.8261\n",
      "INFO:root:Step 20, Loss: 5.0928\n",
      "INFO:root:Step 30, Loss: 5.2674\n",
      "INFO:root:Step 40, Loss: 4.0199\n",
      "INFO:root:Step 50, Loss: 3.6182\n",
      "INFO:root:Step 60, Loss: 2.2397\n",
      "INFO:root:Step 70, Loss: 2.5552\n",
      "INFO:root:Step 80, Loss: 2.9768\n",
      "INFO:root:Step 90, Loss: 3.0076\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Embedding quantization is only supported with float_qparams_weight_only_qconfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m quantized_model = \u001b[43mtrain_qat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_qat\u001b[39m\u001b[34m(model, tokenizer, dataset, num_epochs, max_steps)\u001b[39m\n\u001b[32m     23\u001b[39m             logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Convert to quantized model after training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:655\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m    654\u001b[39m     module = copy.deepcopy(module)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[32m    664\u001b[39m     _remove_qconfig(module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:712\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m     reassign[name] = swap_module(\n\u001b[32m    721\u001b[39m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[32m    722\u001b[39m     )\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:712\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m     reassign[name] = swap_module(\n\u001b[32m    721\u001b[39m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[32m    722\u001b[39m     )\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:720\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    709\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[32m    710\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[32m    711\u001b[39m     ):\n\u001b[32m    712\u001b[39m         _convert(\n\u001b[32m    713\u001b[39m             mod,\n\u001b[32m    714\u001b[39m             mapping,\n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m             use_precomputed_fake_quant=use_precomputed_fake_quant,\n\u001b[32m    719\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     reassign[name] = \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n\u001b[32m    725\u001b[39m     module._modules[key] = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:762\u001b[39m, in \u001b[36mswap_module\u001b[39m\u001b[34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    760\u001b[39m sig = inspect.signature(qmod.from_float)\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_precomputed_fake_quant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig.parameters:\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m     new_mod = \u001b[43mqmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    766\u001b[39m     new_mod = qmod.from_float(mod)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\jarvis\\venv\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\embedding_ops.py:229\u001b[39m, in \u001b[36mEmbedding.from_float\u001b[39m\u001b[34m(cls, mod, use_precomputed_fake_quant)\u001b[39m\n\u001b[32m    224\u001b[39m dtype = weight_observer.dtype\n\u001b[32m    225\u001b[39m is_float_qparams_qconfig = (\n\u001b[32m    226\u001b[39m     weight_observer.qscheme == torch.per_channel_affine_float_qparams\n\u001b[32m    227\u001b[39m )\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     is_float_qparams_qconfig\n\u001b[32m    230\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mEmbedding quantization is only supported with float_qparams_weight_only_qconfig.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    233\u001b[39m     dtype == torch.quint8 \u001b[38;5;129;01mor\u001b[39;00m dtype == torch.quint4x2\n\u001b[32m    234\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe only supported dtype for nnq.Embedding is torch.quint8 and torch.quint4x2, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# Run the observer to calculate qparams.\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: Embedding quantization is only supported with float_qparams_weight_only_qconfig."
     ]
    }
   ],
   "source": [
    "quantized_model = train_qat(model, tokenizer, dataset, num_epochs=1, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44259471",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_qat_model(quantized_model, tokenizer, dataset, limit=200)\n",
    "print(f\"Accuracy on MedMCQA after QAT: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ecf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, limit=200):\n",
    "    \"\"\"Generic evaluation function that works for both original and quantized models\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataset[\"validation\"]):\n",
    "            if i >= limit:\n",
    "                break\n",
    "\n",
    "            question = batch[\"question\"]\n",
    "            options = [batch[\"opa\"], batch[\"opb\"], batch[\"opc\"], batch[\"opd\"]]\n",
    "            answer_idx = batch[\"cop\"]\n",
    "\n",
    "            prompt = f\"Question: {question}\\nOptions:\\nA. {options[0]}\\nB. {options[1]}\\nC. {options[2]}\\nD. {options[3]}\\nAnswer:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "            \n",
    "            outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "            pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answer_letter = pred_text.strip().upper().split(\"Answer:\")[-1].strip()[0]\n",
    "\n",
    "            label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "            if answer_letter in label_map:\n",
    "                predictions.append(label_map[answer_letter])\n",
    "            else:\n",
    "                predictions.append(None)\n",
    "\n",
    "            references.append(answer_idx)\n",
    "\n",
    "    # Filter out None predictions\n",
    "    filtered = [(p, r) for p, r in zip(predictions, references) if p is not None]\n",
    "    predictions, references = zip(*filtered) if filtered else ([], [])\n",
    "\n",
    "    accuracy = accuracy_score(references, predictions) if predictions else 0\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate original model BEFORE QAT\n",
    "original_accuracy = evaluate_model(original_model, tokenizer, dataset, limit=200)\n",
    "print(f\"Accuracy on MedMCQA BEFORE QAT: {original_accuracy:.2%}\")\n",
    "\n",
    "\n",
    "# Evaluate after QAT\n",
    "quantized_accuracy = evaluate_model(quantized_model, tokenizer, dataset, limit=200)\n",
    "print(f\"Accuracy on MedMCQA AFTER QAT: {quantized_accuracy:.2%}\")\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Original model accuracy: {original_accuracy:.2%}\")\n",
    "print(f\"Quantized model accuracy: {quantized_accuracy:.2%}\")\n",
    "print(f\"Accuracy difference: {(quantized_accuracy - original_accuracy):.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e00cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
